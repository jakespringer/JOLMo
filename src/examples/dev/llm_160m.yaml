model:
  _CLASS_: olmo_core.nn.transformer.config.TransformerConfig
  d_model: 768
  vocab_size: 32064
  n_layers: 16
  block:
    _CLASS_: olmo_core.nn.transformer.config.TransformerBlockConfig
    name: reordered_norm
    attention:
      _CLASS_: olmo_core.nn.attention.AttentionConfig
      n_heads: 12
      n_kv_heads: null
      bias: false
      rope:
        _CLASS_: olmo_core.nn.rope.RoPEConfig
        name: default
        theta: 500000
      dtype: float32
    feed_forward:
      _CLASS_: olmo_core.nn.feed_forward.FeedForwardConfig
      hidden_size: 2048
      bias: false
      dtype: float32
    layer_norm:
      _CLASS_: olmo_core.nn.layer_norm.LayerNormConfig
      name: rms
      eps: 1.0e-05
      bias: false
  lm_head:
    _CLASS_: olmo_core.nn.lm_head.LMHeadConfig
    name: default
    bias: false
    dtype: float32
  dtype: float32

dataset:
  _CLASS_: olmo_core.data.NumpyFSLDatasetConfig
  tokenizer:
    _CLASS_: olmo_core.data.TokenizerConfig
    identifier: gpt2
  paths:
    - /path/to/dclm-train.npy
  sequence_length: 2048
  work_dir: /tmp/dataset-cache

data_loader:
  _CLASS_: olmo_core.data.NumpyDataLoaderConfig
  global_batch_size: 262144
  seed: 0
  num_workers: 4

train_module_type: normal
train_module:
  _CLASS_: olmo_core.train.train_module.transformer.config.TransformerTrainModuleConfig
  rank_microbatch_size: 32768
  max_sequence_length: 2048
  optim:
    _CLASS_: olmo_core.optim.AdamWConfig
    lr: 0.0005
    weight_decay: 0.1
    betas: [0.9, 0.95]
    group_overrides:
      - _CLASS_: olmo_core.optim.OptimGroupOverride
        params: [embeddings.weight]
        opts:
          weight_decay: 0.0
    fused: true
  scheduler:
    _CLASS_: olmo_core.optim.CosWithWarmup
    warmup_steps: 2000
  compile_model: true
  dp_config:
    _CLASS_: olmo_core.train.train_module.transformer.config.TransformerDataParallelConfig
    name: fsdp
    param_dtype: bfloat16
    reduce_dtype: float32
  autocast_precision: bfloat16
  max_grad_norm: 1.0

trainer:
  _CLASS_: olmo_core.train.config.TrainerConfig
  save_folder: /tmp/llm_150m
  save_overwrite: true
  metrics_collect_interval: 10
  cancel_check_interval: 5
  callbacks:
    checkpointer:
      _CLASS_: olmo_core.train.callbacks.checkpointer.CheckpointerCallback
      save_interval: 1000
      ephemeral_save_interval: 100
      save_async: true
    config_saver:
      _CLASS_: olmo_core.train.callbacks.config_saver.ConfigSaverCallback
    lm_evaluator:
      _CLASS_: olmo_core.train.callbacks.evaluator_callback.LMEvaluatorCallbackConfig
      eval_dataset:
        _CLASS_: olmo_core.data.NumpyPaddedFSLDatasetConfig
        paths:
          - /path/to/dclm-valid.npy
        metadata:
          - label: dclm-validation
        sequence_length: 2048
        tokenizer:
          _CLASS_: olmo_core.data.TokenizerConfig
          identifier: gpt2
        work_dir: /tmp/dataset-cache
      eval_interval: 1000

wandb:
  enabled: false

init_seed: 12536
load_path: null

